{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3369271-ac80-4029-b531-31fd800882fc",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91bb93-288a-4256-a751-cdf0a2c68eba",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cd0b5-1d1c-4d69-8fe0-be1d54e13ee3",
   "metadata": {},
   "source": [
    "#### Question 1: What is a Decision Tree, and how does it work in the context of classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5440f04-98b8-4ef3-905c-306b71c644a1",
   "metadata": {},
   "source": [
    "- A decision tree is a flowchart-like structure where:\n",
    "\n",
    "    - Each internal node represents a decision based on a feature (e.g., \"Is age > 30?\").\n",
    "\n",
    "    - Each branch represents the outcome of that decision (Yes/No or a split based on values).\n",
    "\n",
    "    - Each leaf node represents the final prediction (a class label in classification).\n",
    "\n",
    "It works by recursively splitting the dataset into smaller and smaller groups based on the feature that best separates the data until a stopping condition is reached (like maximum depth, minimum samples, or pure class distribution).\n",
    "\n",
    "\n",
    "The algorithm tries to partition the dataset so that each group (leaf) contains mostly instances of a single class.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Start with the full dataset as the root.\n",
    "\n",
    "2. Choose the best feature to split the data ‚Äî the one that creates the \"purest\" child nodes.\n",
    "\n",
    "- Metrics used:\n",
    "\n",
    "    - Gini Impurity\n",
    "\n",
    "    - Entropy / Information Gain\n",
    "\n",
    "    - Chi-Square\n",
    "\n",
    "3. Split the dataset into subsets based on the chosen feature‚Äôs values.\n",
    "\n",
    "4. Repeat recursively on each subset until:\n",
    "\n",
    "    - All samples in a node belong to the same class, or\n",
    "\n",
    "    - A maximum depth is reached, or\n",
    "\n",
    "    - No further improvement can be made.\n",
    "\n",
    "5.Assign a class label to each leaf node (majority class in that subset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b99343-9903-4fe9-8e11-fd2934de9a7d",
   "metadata": {},
   "source": [
    "#### Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93953530-4967-4053-b8ae-43f5dfcabae0",
   "metadata": {},
   "source": [
    "- Gini Impurity\n",
    "\n",
    "Definition: Probability of incorrectly classifying a randomly chosen sample if it was randomly labeled according to the class distribution in the node.\n",
    "\n",
    "Gini=1‚àí\n",
    "i=1‚àëCpi2\n",
    "\n",
    "Where:\n",
    "\n",
    "pi= proportion of samples belonging to class \n",
    "\n",
    "C = total number of classes.\n",
    "\n",
    "Example: Suppose a node has:\n",
    "\n",
    "70% samples of Class A (pA=0.7)\n",
    "\n",
    "30% samples of Class B (ùëùùêµ=0.3)\n",
    "\n",
    "If node has all samples of one class ‚Üí Gini = 0 (pure).\n",
    "\n",
    "Maximum Gini occurs when classes are evenly split (e.g., 50-50 ‚Üí Gini = 0.5).\n",
    "\n",
    "\n",
    "\n",
    "- Entropy (Information Gain)\n",
    "\n",
    "Definition: Measures the amount of uncertainty or disorder in a node.\n",
    "Entropy=‚àí‚àëCpi‚ãÖlog2(pi)\n",
    "Where:\n",
    "pi= proportion of samples in class i.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f179b64-b7f0-48d2-b892-98ca9e44bb6e",
   "metadata": {},
   "source": [
    "#### Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1b5f3-a69c-49c7-b5db-37cc2dc96ea3",
   "metadata": {},
   "source": [
    "- Pre-Pruning (Early Stopping)\n",
    "\n",
    "    - Also called early stopping.\n",
    "\n",
    "    - The tree stops growing before it becomes too complex.\n",
    "\n",
    "    - We set constraints while building the tree.\n",
    "\n",
    "Examples of Pre-Pruning techniques:\n",
    "\n",
    "    - Limit the maximum depth of the tree (max_depth).\n",
    "\n",
    "    - Require a minimum number of samples per node (min_samples_split, min_samples_leaf).\n",
    "\n",
    "    - Set a maximum number of leaf nodes (max_leaf_nodes).\n",
    "\n",
    "    - Stop splitting if impurity reduction (Gini/Entropy) is below a threshold.\n",
    "\n",
    "\n",
    "- Post-Pruning (Cost Complexity Pruning)\n",
    "\n",
    "    - The tree is allowed to grow fully (possibly overfitting).\n",
    "\n",
    "    - Afterward, we prune back the tree by removing branches that do not improve performance on validation data.\n",
    "\n",
    "How it works:\n",
    "\n",
    "    - Build a deep tree.\n",
    "\n",
    "    - Use a validation set or cross-validation to evaluate subtrees.\n",
    "\n",
    "    - Iteratively remove nodes/branches that give the least improvement in accuracy or increase in error.\n",
    "\n",
    "    - The best pruned subtree is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec1f66-31d5-40f8-a855-7a717a8c7084",
   "metadata": {},
   "source": [
    "#### Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16e680-ae4a-45e3-a1be-12169c1fac43",
   "metadata": {},
   "source": [
    "- Information Gain is a measure of how much ‚Äúknowledge‚Äù a feature gives us about the class labels.\n",
    "\n",
    "In Decision Trees, when we split on a feature, we want the resulting child nodes to be as pure (homogeneous) as possible.\n",
    "\n",
    "    - If a split reduces the disorder (impurity) a lot ‚Üí high information gain.\n",
    "\n",
    "    - If a split doesn‚Äôt change much ‚Üí low information gain.\n",
    "\n",
    "It is calculated using Entropy.\n",
    "\n",
    "Information Gain in Decision Tree important because:\n",
    "\n",
    "    - At each step, a decision tree algorithm chooses the feature that provides the highest Information Gain.\n",
    "\n",
    "    - This ensures the split maximally reduces impurity and increases class homogeneity in the child nodes.\n",
    "\n",
    "    - Repeating this process recursively leads to a tree that efficiently separates classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6deb3-d11c-4c87-a7e2-de0de805d7e7",
   "metadata": {},
   "source": [
    "#### Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1efd0-893a-42d2-814c-8e75a3ed91a4",
   "metadata": {},
   "source": [
    "Real-World Applications of Decision Trees\n",
    "\n",
    "1. Healthcare\n",
    "\n",
    "    - Diagnosing diseases (e.g., ‚ÄúDoes the patient have diabetes?‚Äù based on symptoms, test results).\n",
    "\n",
    "    - Predicting patient outcomes (survival, risk factors).\n",
    "\n",
    "2. Finance\n",
    "\n",
    "    - Credit risk assessment (approve/reject a loan).\n",
    "\n",
    "    - Fraud detection in banking transactions.\n",
    "\n",
    "3. Marketing & Business\n",
    "\n",
    "    - Customer segmentation (who is likely to buy a product).\n",
    "\n",
    "    - Churn prediction (which customers may stop using a service).\n",
    "\n",
    "4. Retail & E-commerce\n",
    "\n",
    "    - Recommendation systems (products based on past behavior).\n",
    "\n",
    "    - Price prediction for items.\n",
    "\n",
    "5. Manufacturing & Operations\n",
    "\n",
    "    - Predicting equipment failure (maintenance scheduling).\n",
    "\n",
    "    - Quality control decisions.\n",
    "\n",
    "6. Government & Law\n",
    "\n",
    "    - Crime prediction (identifying high-risk areas).\n",
    "\n",
    "    - Tax fraud detection.Healthcare\n",
    "\n",
    "\n",
    "                              \n",
    "Main Advantages of Decision Trees\n",
    "\n",
    "1. Simple and Interpretable\n",
    "\n",
    "    - Easy to visualize and explain (‚Äúwhite-box model‚Äù).\n",
    "\n",
    "    - Non-technical stakeholders can understand rules.\n",
    "\n",
    "2. Handles Different Data Types\n",
    "\n",
    "    - Works with both categorical and numerical data.\n",
    "\n",
    "3. Requires Little Data Preparation\n",
    "\n",
    "    - No need for feature scaling (like standardization/normalization).\n",
    "\n",
    "    - Handles missing values (depending on implementation).\n",
    "\n",
    "4. Fast Training and Prediction\n",
    "\n",
    "    - Splitting is straightforward and computationally efficient.\n",
    "\n",
    "5. Versatility\n",
    "\n",
    "    - Can be used for classification and regression problems.\n",
    "\n",
    "\n",
    "\n",
    "Main Limitations of Decision Trees\n",
    "\n",
    "1. Overfitting\n",
    "\n",
    "    - If grown too deep, trees capture noise and lose generalization power.\n",
    "\n",
    "2. Instability\n",
    "\n",
    "    - Small changes in data can produce very different trees (high variance).\n",
    "\n",
    "3. Biased Splits\n",
    "\n",
    "    - Features with many levels (e.g., unique IDs) may dominate splits.\n",
    "\n",
    "4. Limited Predictive Accuracy\n",
    "\n",
    "    - Alone, decision trees are often less accurate than ensemble methods like Random Forest or Gradient Boosted Trees.\n",
    "\n",
    "5. Not Good with Continuous Boundaries\n",
    "\n",
    "    - Struggles with smooth decision boundaries (linear classifiers often do better here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff4143-1e04-4a94-99e7-09824052046b",
   "metadata": {},
   "source": [
    "#### Question 6: Write a Python program to:\n",
    "#### ‚óè Load the Iris Dataset\n",
    "#### ‚óè Train a Decision Tree Classifier using the Gini criterion\n",
    "#### ‚óè Print the model‚Äôs accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24dda6e-d0c6-4b40-aab6-284f001dc095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0167\n",
      "petal length (cm): 0.9061\n",
      "petal width (cm): 0.0772\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create Decision Tree Classifier using Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
    "    print(f\"{feature_name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7b1da-45bd-4713-ab7c-3ae4d35236bc",
   "metadata": {},
   "source": [
    "#### Question 7: Write a Python program to:\n",
    "#### ‚óè Load the Iris Dataset\n",
    "#### ‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d23a6af-bf6f-4219-9409-f1d31c229c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fully-grown tree: 1.0\n",
      "Accuracy of pruned tree (max_depth=3): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Fully-grown Decision Tree (no depth limit) ---\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "# --- Pruned Decision Tree (max_depth=3) ---\n",
    "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_pruned.fit(X_train, y_train)\n",
    "y_pred_pruned = clf_pruned.predict(X_test)\n",
    "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy of fully-grown tree:\", accuracy_full)\n",
    "print(\"Accuracy of pruned tree (max_depth=3):\", accuracy_pruned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd685ff-b84c-4280-b69e-dab17068a049",
   "metadata": {},
   "source": [
    "#### Question 8: Write a Python program to:\n",
    "#### ‚óè Load the California Housing dataset from sklearn\n",
    "#### ‚óè Train a Decision Tree Regressor\n",
    "#### ‚óè Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498e9ea1-8f62-451d-8351-321cb67cc981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.495235205629094\n",
      "\n",
      "Feature Importances:\n",
      "MedInc: 0.5285\n",
      "HouseAge: 0.0519\n",
      "AveRooms: 0.0530\n",
      "AveBedrms: 0.0287\n",
      "Population: 0.0305\n",
      "AveOccup: 0.1308\n",
      "Latitude: 0.0937\n",
      "Longitude: 0.0829\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a Decision Tree Regressor\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature_name, importance in zip(housing.feature_names, reg.feature_importances_):\n",
    "    print(f\"{feature_name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ff471-6389-4451-abe6-5cc16cc3bb21",
   "metadata": {},
   "source": [
    "#### Question 9: Write a Python program to: \n",
    "#### ‚óè Load the Iris Dataset \n",
    "#### ‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using GridSearchCV \n",
    "#### ‚óè Print the best parameters and the resulting model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959bc43b-93a7-48e1-a087-4848bdb6e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
      "Test Accuracy with Best Parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the Decision Tree model\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [2, 3, 4, 5, None],          # tree depth options\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 10]     # min samples per split\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV (5-fold cross-validation)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy with Best Parameters:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c468bd-bc37-4168-882f-551c783f58a6",
   "metadata": {},
   "source": [
    "#### Question 10: Imagine you‚Äôre working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
    "#### Explain the step-by-step process you would follow to:\n",
    "#### ‚óè Handle the missing values\n",
    "#### ‚óè Encode the categorical features\n",
    "#### ‚óè Train a Decision Tree model\n",
    "#### ‚óè Tune its hyperparameters\n",
    "#### ‚óè Evaluate its performance\n",
    "#### And describe what business value this model could provide in the real-world setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5df091-1348-4b44-bd1b-8385c1b22a88",
   "metadata": {},
   "source": [
    "1. Handle missing values ‚Äî step by step\n",
    "\n",
    "- Assess the missingness\n",
    "\n",
    "    - Compute % missing per column; visualize patterns (heatmap, missingno, or df.isna().sum()).\n",
    "\n",
    "    - Try to determine mechanism: MCAR / MAR / MNAR ‚Äî this influences strategy.\n",
    "\n",
    "- Decide to drop or impute\n",
    "\n",
    "    - Drop a feature if > threshold missing (e.g., >60‚Äì80%) and it‚Äôs not critical.\n",
    "\n",
    "    - Drop rows only if very few and missingness appears random.\n",
    "\n",
    "- Impute carefully (avoid leakage)\n",
    "\n",
    "    - Use SimpleImputer (mean/median for numeric, most_frequent for categorical) for a baseline.\n",
    "\n",
    "    - For better quality, use KNNImputer or IterativeImputer (model-based).\n",
    "\n",
    "    - Add a missing indicator column when ‚Äúmissingness‚Äù may itself be predictive.\n",
    "\n",
    "- Always fit imputers on training data only (use Pipeline/ColumnTransformer).\n",
    "\n",
    "\n",
    "2. Encode categorical features\n",
    "\n",
    "- Low-cardinality (<= ~10) ‚Üí OneHotEncoder(drop='first').\n",
    "\n",
    "- High-cardinality ‚Üí target encoding / hashing / leave-one-out / embedding. Beware target leakage (do target encoding with cross fold or within CV).\n",
    "\n",
    "- Label/Ordinal encoding: trees can split on integer values, but label encoding may introduce spurious order ‚Äî use only if category has real order.\n",
    "\n",
    "- Consider algorithms that handle categoricals natively (CatBoost / LightGBM) if you want to avoid heavy encodings.\n",
    "\n",
    "\n",
    "3. Training a Decision Tree (best practice)\n",
    "\n",
    "- Use a Pipeline that includes preprocessing (imputation + encoding) and the classifier.\n",
    "\n",
    "- Use class_weight='balanced' or sample weights if the disease is rare.\n",
    "\n",
    "- Use stratified splitting (StratifiedKFold, train_test_split(..., stratify=y)) to preserve prevalence in train/test.\n",
    "\n",
    "\n",
    "4. Hyperparameter tuning\n",
    "\n",
    "- Tune important parameters:\n",
    "\n",
    "    - max_depth, min_samples_split, min_samples_leaf, max_features, ccp_alpha (cost-complexity pruning), criterion (gini/entropy),class_weight.\n",
    "\n",
    "- Use GridSearchCV or RandomizedSearchCV with StratifiedKFold.\n",
    "\n",
    "- Choose scoring aligned with business objective (e.g., recall/sensitivity if false negatives are expensive, average_precision / PR-AUC for severe class imbalance, or ROC-AUC for overall ranking).\n",
    "\n",
    "- Consider nested CV for unbiased generalization-error estimates if you report a final score.\n",
    "\n",
    "\n",
    "5. Evaluation (what to compute & why)\n",
    "\n",
    "- Primary metrics (choose per business need):\n",
    "\n",
    "    - Sensitivity (Recall), Specificity, Precision, F1.\n",
    "\n",
    "    - ROC-AUC and PR-AUC (PR-AUC is more informative with rare disease).\n",
    "\n",
    "    - Confusion matrix at chosen threshold.\n",
    "\n",
    "- Probability calibration: Decision Trees are poorly calibrated; use CalibratedClassifierCV (isotonic or sigmoid) if you need reliable probabilities for risk scoring.\n",
    "\n",
    "- Threshold tuning: pick a decision threshold that balances FN/FPs based on cost matrix.\n",
    "\n",
    "- Explainability: show global feature_importances_ and run SHAP or LIME for per-patient explanations.\n",
    "\n",
    "- Fairness & subgroup analysis: check performance across age, gender, ethnicity subgroups.\n",
    "\n",
    "\n",
    "6. Business value & real-world considerations\n",
    "\n",
    "- Clinical value: early detection, prioritize high-risk patients, target diagnostic testing, reduce time-to-treatment.\n",
    "\n",
    "- Operational value: improve allocation of scarce resources (specialist visits, tests), reduce downstream costs by early intervention.\n",
    "\n",
    "- Risk management: tune for high recall if missing disease is costly; use human-in-the-loop to verify positive predictions.\n",
    "\n",
    "- Trust & adoption: provide explanations (SHAP), plot decision rules for clinicians, and pilot before full deployment.\n",
    "\n",
    "- Governance: ensure privacy (HIPAA/PDPA), audit model for bias, maintain retraining/monitoring pipeline to detect drift.\n",
    "\n",
    "- Deployment approach: start as decision-support (assistive), not autonomous diagnosis; measure clinical outcomes via an A/B or prospective study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd0d9d-3c2c-4a15-84c0-fefe32ac7e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load dataset as DataFrame\n",
    "# -----------------------------\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Train-test split (stratified)\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Identify numeric/categorical columns\n",
    "# -----------------------------\n",
    "numeric_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object','category']).columns.tolist()  # none in iris\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Preprocessing pipelines\n",
    "# -----------------------------\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline, numeric_features),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Full pipeline with Decision Tree\n",
    "# -----------------------------\n",
    "pipe = Pipeline([\n",
    "    (\"preproc\", preprocessor),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Hyperparameter grid for tuning\n",
    "# -----------------------------\n",
    "param_grid = {\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"clf__max_depth\": [3, 5, 8, None],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 5],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "    \"clf__ccp_alpha\": [0.0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(pipe, param_grid, cv=cv, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Best parameters\n",
    "# -----------------------------\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Evaluate on test set\n",
    "# ------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c935535-afff-4ecd-9b0a-09a34f36b006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
